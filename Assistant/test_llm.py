#!/usr/bin/env python3
"""
Prosty skrypt testowy do sprawdzenia czy LLM dziaÅ‚a.
"""

import requests
import json

def test_ollama():
    """Test poÅ‚Ä…czenia z Ollama."""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            print("âœ“ Ollama dziaÅ‚a!")
            print(f"âœ“ DostÄ™pne modele: {[m['name'] for m in models]}")
            return True
        else:
            print("âœ— Ollama nie odpowiada")
            return False
    except Exception as e:
        print(f"âœ— BÅ‚Ä…d poÅ‚Ä…czenia z Ollama: {e}")
        print("  Upewnij siÄ™, Å¼e Ollama dziaÅ‚a: ollama serve")
        return False

def test_server():
    """Test serwera LLM."""
    try:
        response = requests.get("http://localhost:8000/", timeout=5)
        if response.status_code == 200:
            print("âœ“ Serwer LLM dziaÅ‚a!")
            print(f"  Status: {response.json()}")
            return True
        else:
            print("âœ— Serwer nie odpowiada")
            return False
    except Exception as e:
        print(f"âœ— Serwer nie dziaÅ‚a: {e}")
        print("  Uruchom serwer: python integrated_server.py")
        return False

def test_chat():
    """Test rozmowy z LLM."""
    try:
        print("\nğŸ“ Test rozmowy z LLM...")
        response = requests.post(
            "http://localhost:8000/chat",
            json={
                "messages": [
                    {"role": "user", "content": "CzeÅ›Ä‡! Jak siÄ™ masz?"}
                ],
                "include_traffic_info": False  # Bez kontekstu o komunikacji na razie
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            print("âœ“ LLM odpowiedziaÅ‚!")
            print(f"\nOdpowiedÅº: {result['message'][:200]}...")
            return True
        else:
            print(f"âœ— BÅ‚Ä…d: {response.status_code}")
            print(response.text)
            return False
    except Exception as e:
        print(f"âœ— BÅ‚Ä…d: {e}")
        return False

def test_traffic_chat():
    """Test rozmowy z LLM z kontekstem o komunikacji."""
    try:
        print("\nğŸšŒ Test rozmowy z LLM o komunikacji...")
        response = requests.post(
            "http://localhost:8000/chat",
            json={
                "messages": [
                    {"role": "user", "content": "Jakie sÄ… aktualne zmiany w rozkÅ‚adach jazdy?"}
                ],
                "include_traffic_info": True
            },
            timeout=120  # DÅ‚uÅ¼szy timeout bo LLM musi przetworzyÄ‡ duÅ¼o kontekstu
        )
        
        if response.status_code == 200:
            result = response.json()
            print("âœ“ LLM odpowiedziaÅ‚ na pytanie o komunikacjÄ™!")
            print(f"\nOdpowiedÅº: {result['message'][:300]}...")
            return True
        else:
            print(f"âœ— BÅ‚Ä…d: {response.status_code}")
            print(response.text)
            return False
    except Exception as e:
        print(f"âœ— BÅ‚Ä…d: {e}")
        return False

if __name__ == "__main__":
    print("=" * 60)
    print("TEST SYSTEMU LLM")
    print("=" * 60)
    
    # Test 1: Ollama
    print("\n1. Test Ollama...")
    ollama_ok = test_ollama()
    
    if not ollama_ok:
        print("\nâŒ Ollama nie dziaÅ‚a. Uruchom: ollama serve")
        exit(1)
    
    # Test 2: Serwer
    print("\n2. Test serwera LLM...")
    server_ok = test_server()
    
    if not server_ok:
        print("\nâŒ Serwer nie dziaÅ‚a. Uruchom w osobnym terminalu:")
        print("   cd Assistant")
        print("   python integrated_server.py")
        exit(1)
    
    # Test 3: Podstawowa rozmowa
    print("\n3. Test podstawowej rozmowy...")
    chat_ok = test_chat()
    
    # Test 4: Rozmowa o komunikacji
    if chat_ok:
        print("\n4. Test rozmowy o komunikacji...")
        test_traffic_chat()
    
    print("\n" + "=" * 60)
    print("Test zakoÅ„czony!")
    print("=" * 60)

